{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a68df0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9b7bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "34260ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0        \n",
    "            \n",
    "    # selection phase\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        \n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "                    \n",
    "    # expansion phase\n",
    "    def expand(self, policy):\n",
    "        for action, probability in enumerate(policy):\n",
    "            if probability > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, probability)\n",
    "                self.children.append(child)       \n",
    "                \n",
    "        return child        \n",
    "        \n",
    "    # backpropagation phase   \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "867242e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1\n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.in_a_row - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3e1a983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        # define root node\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "        \n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # selection phase\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                # expansion phase\n",
    "                node.expand(policy)\n",
    "            # backpropagation phase\n",
    "            node.backpropagate(value)\n",
    "        # return visit_counts\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d55f5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, states, spGames):\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
    "        \n",
    "        for i, spg in enumerate(spGames):\n",
    "            spg_policy = policy[i]\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "                \n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "                    \n",
    "                else:\n",
    "                    spg.node = node\n",
    "                    \n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "                    \n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "                \n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "                \n",
    "            for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                node = spGames[mappingIdx].node\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "                \n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "                node.expand(spg_policy)\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d6dec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "        \n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = func.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = self.batchnorm2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = func.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5d237ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "    def self_play(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "            \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                     ))\n",
    "                return returnMemory\n",
    "\n",
    "            player = self.game.get_opponent(player)\n",
    "            \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:batchIdx+self.args['batch_size']]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = func.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = func.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for self_play_iteration in trange(self.args['num_self_play_iterations']):\n",
    "                memory += self.self_play()\n",
    "            \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3505799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "        \n",
    "    def self_play(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
    "        \n",
    "        while len(spGames) > 0:\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "            \n",
    "            self.mcts.search(neutral_states, spGames)\n",
    "            \n",
    "            for i in range(len(spGames))[::-1]:\n",
    "                spg = spGames[i]\n",
    "                \n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs) \n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "                    del spGames[i]\n",
    "                    \n",
    "            player = self.game.get_opponent(player)\n",
    "            \n",
    "        return return_memory\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:batchIdx+self.args['batch_size']]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = func.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = func.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for self_play_iteration in trange(self.args['num_self_play_iterations'] // self.args['num_parallel_games']):\n",
    "                memory += self.self_play()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
    "            \n",
    "class SPG:\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7039789e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:23<00:00,  1.90it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_184113/3764908677.py:56: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = func.mse_loss(out_value, value_targets)\n",
      "/tmp/ipykernel_184113/3764908677.py:56: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = func.mse_loss(out_value, value_targets)\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.06it/s]\n",
      "100%|██████████| 500/500 [03:53<00:00,  2.14it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_184113/3764908677.py:56: UserWarning: Using a target size (torch.Size([57])) that is different to the input size (torch.Size([57, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = func.mse_loss(out_value, value_targets)\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.05it/s]\n",
      "100%|██████████| 500/500 [03:18<00:00,  2.52it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_184113/3764908677.py:56: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = func.mse_loss(out_value, value_targets)\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet(ttt, 4, 64, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iterations': 3,\n",
    "    'num_self_play_iterations': 500,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': .25,\n",
    "    'dirichlet_alpha': .3\n",
    "}\n",
    "\n",
    "alphaZP = AlphaZeroParallel(model, optimizer, ttt, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6f3a882",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05238787457346916\n",
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "tensor([[[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 0., 1.]]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, -1)\n",
    "state = tictactoe.get_next_state(state, 4, -1)\n",
    "state = tictactoe.get_next_state(state, 6, 1)\n",
    "state = tictactoe.get_next_state(state, 8, 1)\n",
    "\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device=device)\n",
    "model.load_state_dict(torch.load('model_2.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58ea1db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [2:39:04<00:00, 1908.91s/it]  \n",
      "100%|██████████| 4/4 [02:41<00:00, 40.26s/it]\n",
      "100%|██████████| 5/5 [2:13:49<00:00, 1605.91s/it]\n",
      "100%|██████████| 4/4 [02:45<00:00, 41.49s/it]\n",
      "100%|██████████| 5/5 [2:24:33<00:00, 1734.79s/it]\n",
      "100%|██████████| 4/4 [03:01<00:00, 45.30s/it]\n",
      "100%|██████████| 5/5 [2:33:21<00:00, 1840.29s/it]  \n",
      "100%|██████████| 4/4 [03:12<00:00, 48.04s/it]\n",
      "100%|██████████| 5/5 [2:54:33<00:00, 2094.72s/it]  \n",
      "100%|██████████| 4/4 [03:33<00:00, 53.36s/it]\n",
      "100%|██████████| 5/5 [2:58:29<00:00, 2141.96s/it]  \n",
      "100%|██████████| 4/4 [03:41<00:00, 55.38s/it]\n",
      "100%|██████████| 5/5 [3:10:57<00:00, 2291.46s/it]  \n",
      "100%|██████████| 4/4 [03:38<00:00, 54.62s/it]\n",
      " 20%|██        | 1/5 [1:00:43<4:02:55, 3643.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 23\u001b[0m\n\u001b[1;32m      9\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_searches\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m600\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirichlet_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m     20\u001b[0m }\n\u001b[1;32m     22\u001b[0m alphaZP \u001b[38;5;241m=\u001b[39m AlphaZeroParallel(model, optimizer, game, args)\n\u001b[0;32m---> 23\u001b[0m alphaZP\u001b[38;5;241m.\u001b[39mlearn()\n",
      "Cell \u001b[0;32mIn[62], line 79\u001b[0m, in \u001b[0;36mAlphaZeroParallel.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m selfPlay_iteration \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_self_play_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_parallel_games\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 79\u001b[0m     memory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_play()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[62], line 18\u001b[0m, in \u001b[0;36mAlphaZeroParallel.self_play\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([spg\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;28;01mfor\u001b[39;00m spg \u001b[38;5;129;01min\u001b[39;00m spGames])\n\u001b[1;32m     16\u001b[0m neutral_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mchange_perspective(states, player)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmcts\u001b[38;5;241m.\u001b[39msearch(neutral_states, spGames)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(spGames))[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     21\u001b[0m     spg \u001b[38;5;241m=\u001b[39m spGames[i]\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[64], line 46\u001b[0m, in \u001b[0;36mMCTSParallel.search\u001b[0;34m(self, states, spGames)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expandable_spGames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     44\u001b[0m     states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([spGames[mappingIdx]\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;28;01mfor\u001b[39;00m mappingIdx \u001b[38;5;129;01min\u001b[39;00m expandable_spGames])\n\u001b[0;32m---> 46\u001b[0m     policy, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m     47\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_encoded_state(states), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     policy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(policy, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     50\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[48], line 39\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstartBlock(x)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resBlock \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackBone:\n\u001b[0;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m resBlock(x)\n\u001b[1;32m     40\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicyHead(x)\n\u001b[1;32m     41\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalueHead(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[48], line 55\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     54\u001b[0m     residual \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 55\u001b[0m     x \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     56\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m     57\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'num_iterations': 8,\n",
    "    'num_self_play_iterations': 500,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZP = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alphaZP.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "409616ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "1:2\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "1:1\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1. -1.  0.  0.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "1:4\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1. -1.  1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1. -1.  1.  0.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "1:4\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  1.  0.  0.]\n",
      " [ 0.  1.  1. -1.  1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  1.  0.  0.]\n",
      " [ 0.  1.  1. -1.  1.  0.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "1:2\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  1.  0.  0.]\n",
      " [ 0.  1.  1. -1.  1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1.  1.  0.  0.]\n",
      " [ 0.  1.  1. -1.  1.  0.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "1:5\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1.  1.  0.  0.]\n",
      " [ 0.  1.  1. -1.  1.  1.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1.  1.  0.  0.]\n",
      " [ 0.  1.  1. -1.  1.  1.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "1:3\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  1.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1.  1.  0.  0.]\n",
      " [ 0.  1.  1. -1.  1.  1.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  1.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1. -1.  0.  0.]\n",
      " [ 0.  0. -1. -1.  1.  0.  0.]\n",
      " [ 0.  1.  1. -1.  1.  1.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "1:0\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  1.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1. -1.  0.  0.]\n",
      " [ 0.  0. -1. -1.  1.  0.  0.]\n",
      " [ 1.  1.  1. -1.  1.  1.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  1.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1. -1.  0.  0.]\n",
      " [ 0. -1. -1. -1.  1.  0.  0.]\n",
      " [ 1.  1.  1. -1.  1.  1.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "1:0\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  1.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1. -1.  1.  0.  0.]\n",
      " [ 1.  1.  1. -1.  1.  1.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  1.  0.  0.  0.]\n",
      " [ 0. -1. -1. -1. -1.  0.  0.]\n",
      " [ 1. -1. -1. -1.  1.  0.  0.]\n",
      " [ 1.  1.  1. -1.  1.  1.  0.]]\n",
      "-1 won\n"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'dirichlet_epsilon': 0.,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "model.load_state_dict(torch.load(\"model_6_ConnectFour.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = game.get_opponent(player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
